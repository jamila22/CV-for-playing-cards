{"cells":[{"cell_type":"markdown","source":["**import packages**"],"metadata":{"id":"5prko4vMhhmW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"meu2d0u958pl"},"outputs":[],"source":["import numpy as np\n","from sklearn.utils import shuffle\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from keras.models import load_model\n","import random\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"markdown","metadata":{"id":"C8uY7Y7o55WW"},"source":["**load data and labels**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nyoEPWal0R7F"},"outputs":[],"source":["d1 = np.load('card_aug_bw1.npy')\n","ls1 = np.load('labs_aug1.npy')\n","\n","d2 = np.load('card_aug_bw2.npy')\n","ls2 = np.load('labs_aug2.npy')\n","\n","d3 = np.load('card_aug_bw3.npy')\n","ls3 = np.load('labs_aug3.npy')\n","\n","d4 = np.load('card_aug_bw4.npy')\n","ls4 = np.load('labs_aug4.npy')\n","\n","d5 = np.load('card_aug_bw5.npy')\n","ls5 = np.load('labs_aug5.npy')\n","\n","d6 = np.load('card_aug_bw6.npy')\n","ls6 = np.load('labs_aug6.npy')\n","\n","d7 = np.load('card_aug_bw7.npy')\n","ls7 = np.load('labs_aug7.npy')\n","\n","d8 = np.load('card_aug_bw8.npy')\n","ls8 = np.load('labs_aug8.npy')\n","\n","d9 = np.load('card_aug_bw9.npy')\n","ls9 = np.load('labs_aug9.npy')\n","\n","d10 = np.load('card_aug_bw10.npy')\n","ls10 = np.load('labs_aug10.npy')\n","\n","tot_data = np.concatenate((d1,d2,d3,d4,d5,d6,d7,d8,d9,d10), axis=0)\n","tot_labs = np.concatenate((ls1,ls2,ls3,ls4,ls5,ls6,ls7,ls8,ls9,ls10), axis=0)\n","\n","tot_data, tot_labs = shuffle(tot_data, tot_labs)"]},{"cell_type":"markdown","metadata":{"id":"TgBb96gk6BXy"},"source":["**preprocess data for deep learning**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTr-eKDt9kzP"},"outputs":[],"source":["train_data, val_data, train_labels, val_labels = train_test_split(tot_data, tot_labs, test_size=0.1, random_state=42)\n","\n","train_data = train_data.reshape(train_data.shape[0], 128, 336, 1) #reshape data for deep learning\n","val_data = val_data.reshape(val_data.shape[0], 128, 336, 1)\n","\n","num_classes = 5\n","\n","one_hot_labels_t = np.eye(num_classes)[train_labels - 14] #convert labels to one hot encoding\n","one_hot_labels_v = np.eye(num_classes)[val_labels - 14]"]},{"cell_type":"markdown","metadata":{"id":"NpvekwI96Tjc"},"source":["**show random samples of the dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LT6wscuYWpQh"},"outputs":[],"source":["idx = random.sample(range(val_data.shape[0]), 40) #select 40 random samples\n","\n","samples =val_data[idx]\n","lab_s = one_hot_labels_v[idx]\n","\n","fig, axs = plt.subplots(5, 8, figsize=(16, 10)) # plot the samples with their labels\n","for i, ax in enumerate(axs.flatten()):\n","    ax.imshow(samples[i], cmap='gray')\n","    label = str(lab_s[i])\n","    ax.set_title(f\"Label: {label}\")\n","    ax.axis('off')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"e1R4WHTO6g-O"},"source":["**define architecture and train model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbLtZdZ66gxS"},"outputs":[],"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Conv2D(8, (21,21), activation='relu', input_shape=(128,336,1)),\n","    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n","    tf.keras.layers.Dropout(0.25),\n","    tf.keras.layers.Conv2D(16, (13,13), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n","    tf.keras.layers.Dropout(0.25),\n","    tf.keras.layers.Conv2D(32, (5,5), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(5, activation='softmax')\n","])\n","\n","\n","learning_rate = 0.001\n","optimizer = Adam(learning_rate=learning_rate)\n","model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","#Train the model and save the training history\n","history = model.fit(train_data, one_hot_labels_t, epochs=45, batch_size=30, validation_data=(val_data, one_hot_labels_v))\n","\n","model.save('my_model6fake.h5') #save trained model\n","\n","plt.plot(history.history['loss'], label='Training Loss') #plot training loss\n","plt.plot(history.history['val_loss'], label='Test loss') #plot validation loss\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss(Suits)')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"-zHkhd_17-pL"},"source":["**display misclassified samples**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kkpKbFjCgWF0"},"outputs":[],"source":["pred_labels = model.predict(val_data)\n","# Convert predictions and true labels to class numbers\n","pred_classes = np.argmax(pred_labels, axis=1)\n","true_classes = np.argmax(one_hot_labels_v, axis=1)\n","# Get misclassified samples\n","misclassified_indices = np.where(pred_classes != true_classes)[0]\n","misclassified_images = val_data[misclassified_indices]\n","misclassified_true_labels = true_classes[misclassified_indices]\n","misclassified_pred_labels = pred_classes[misclassified_indices]\n","\n","# Display misclassified samples with labels\n","for i in range(len(misclassified_indices)):\n","    plt.imshow(misclassified_images[i], cmap='gray')\n","    plt.title(f'True label: {misclassified_true_labels[i]+14}, Predicted label: {misclassified_pred_labels[i]+14}')\n","    plt.show()\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1fmBrRDJVCEq5YyXLBxxsT9q8r8R9HZ0y","timestamp":1711284795133}],"gpuType":"T4","authorship_tag":"ABX9TyPGYCdnl6weCJl5dddYUVq4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}